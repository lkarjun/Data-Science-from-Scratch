{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearalgebra import Vector, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    ''' return 1 if the perceptron \"fires\",  0 if not'''\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_weights = [2., 2]\n",
    "and_bias = -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_output(and_weights, and_bias, [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed- forward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1/(1+math.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    '''weights includes the bias term, inputs includes a 1'''\n",
    "    return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    '''\n",
    "    Feeds the inputs vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).'''\n",
    "    \n",
    "    outputs: List[Vector] = []\n",
    "    \n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]  # Add a constant.\n",
    "        output = [neuron_output(neuron, input_with_bias)\n",
    "                  for neuron in layer]\n",
    "        outputs.append(output)  # add to results.\n",
    "        #  then the input to the next layers is the output of this one\n",
    "        \n",
    "        input_vector = output\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_network = [# hidden layer\n",
    "                [[20., 20, -30],\n",
    "                [20., 20, -10]],\n",
    "                [[-60., 60, -30]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.5397868702434395e-05, 0.9999546021312976], [0.9999999999999059]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(xor_network, [1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]],\n",
    "                      input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [[[random.random() for _ in range(2+1)],\n",
    "            [random.random() for _ in range(2+1)]],\n",
    "            [random.random() for _ in range(2+1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GradientDescent import gradient_step\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor:   0%|                                                                    | 0/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-91ec9b9c651d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"neural net for xor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqerror_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         network = [[gradient_step(neuron, grad, -learning_rate)\n\u001b[0;32m      7\u001b[0m                     for neuron, grad in zip(layer, layer_grad)]\n",
      "\u001b[1;32m<ipython-input-12-3130b5933a59>\u001b[0m in \u001b[0;36msqerror_gradients\u001b[1;34m(network, input_vector, target_vector)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mhidden_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# gradients with respect to output neuron pre-activation outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c164847b820a>\u001b[0m in \u001b[0;36mfeed_forward\u001b[1;34m(neural_network, input_vector)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0minput_with_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_vector\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Add a constant.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         output = [neuron_output(neuron, input_with_bias)\n\u001b[0m\u001b[0;32m     12\u001b[0m                   for neuron in layer]\n\u001b[0;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# add to results.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c164847b820a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0minput_with_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_vector\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Add a constant.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         output = [neuron_output(neuron, input_with_bias)\n\u001b[0m\u001b[0;32m     12\u001b[0m                   for neuron in layer]\n\u001b[0;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# add to results.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2b2acb7203ff>\u001b[0m in \u001b[0;36mneuron_output\u001b[1;34m(weights, inputs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mneuron_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m'''weights includes the bias term, inputs includes a 1'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\desktop\\data-science\\18 Neural Network\\linearalgebra.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(v, w)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mVector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'len of vectors dont match'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_i\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw_i\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.0\n",
    "\n",
    "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "    for x, y in zip(xs, ys):\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                    for neuron, grad in zip(layer, layer_grad)]\n",
    "                    for layer, layer_grad in zip(network, gradients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fizzbuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(x: int) -> Vector:\n",
    "    if x % 15 == 0:\n",
    "        return [0, 0, 0, 1]\n",
    "    elif x % 5 == 0:\n",
    "        return [0, 0, 1, 0]\n",
    "    elif x % 3 == 0:\n",
    "        return [0, 1, 0, 0]\n",
    "    else:\n",
    "        return [1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "[0, 0, 1, 0]\n",
      "[0, 0, 0, 1]\n",
      "[0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(fizz_buzz_encode(2))\n",
    "print(fizz_buzz_encode(3))\n",
    "print(fizz_buzz_encode(5))\n",
    "print(fizz_buzz_encode(15))\n",
    "print(fizz_buzz_encode(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(x: int) -> Vector:\n",
    "    binary: List[float] = []\n",
    "    for i in range(10):\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 1, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(binary_encode(2))\n",
    "print(binary_encode(3))\n",
    "print(binary_encode(5))\n",
    "print(binary_encode(7))\n",
    "print(binary_encode(15))\n",
    "print(binary_encode(1001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [binary_encode(n)\n",
    "      for n in range(101, 1024)]\n",
    "ys = [fizz_buzz_encode(n)\n",
    "      for n in range(101, 1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_HIDDEN = 25\n",
    "\n",
    "network = [# hidden layer: 10 inputs -> Num_hidden outputs\n",
    "           [[random.random() for i in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "           [[random.random() for _ in range(NUM_HIDDEN + 1)]\n",
    "            for _ in range(4)]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearalgebra import squared_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fizz buzz Loss: 30.14: 100%|█████████████████████████████████████████████████████████| 500/500 [20:11<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.0\n",
    "\n",
    "with tqdm.trange(500) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = feed_forward(network, x)[-1]\n",
    "            epoch_loss += squared_distance(predicted, y)\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "            \n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                       for neuron, grad in zip(layer, layer_grad)]\n",
    "                        for layer, layer_grad in zip(network, gradients)]\n",
    "            t.set_description(f\"fizz buzz Loss: {epoch_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(xs: list) -> int:\n",
    "    '''return the index of the largest value'''\n",
    "    return max(range(len(xs)), key= lambda i: xs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "2 2 2\n",
      "3 fizz fizz\n",
      "4 4 4\n",
      "5 buzz buzz\n",
      "6 fizz fizz\n",
      "7 7 7\n",
      "8 8 8\n",
      "9 fizz fizz\n",
      "10 buzz buzz\n",
      "11 11 11\n",
      "12 fizz fizz\n",
      "13 13 13\n",
      "14 14 14\n",
      "15 fizzbuzz fizzbuzz\n",
      "16 16 16\n",
      "17 17 17\n",
      "18 fizz fizz\n",
      "19 19 19\n",
      "20 20 buzz\n",
      "21 fizz fizz\n",
      "22 22 22\n",
      "23 23 23\n",
      "24 fizz fizz\n",
      "25 buzz buzz\n",
      "26 26 26\n",
      "27 fizz fizz\n",
      "28 28 28\n",
      "29 29 29\n",
      "30 fizzbuzz fizzbuzz\n",
      "31 31 31\n",
      "32 32 32\n",
      "33 fizz fizz\n",
      "34 34 34\n",
      "35 buzz buzz\n",
      "36 fizz fizz\n",
      "37 37 37\n",
      "38 38 38\n",
      "39 fizz fizz\n",
      "40 buzz buzz\n",
      "41 41 41\n",
      "42 fizz fizz\n",
      "43 43 43\n",
      "44 44 44\n",
      "45 fizzbuzz fizzbuzz\n",
      "46 46 46\n",
      "47 47 47\n",
      "48 fizz fizz\n",
      "49 49 49\n",
      "50 buzz buzz\n",
      "51 fizz fizz\n",
      "52 52 52\n",
      "53 53 53\n",
      "54 fizz fizz\n",
      "55 buzz buzz\n",
      "56 56 56\n",
      "57 fizz fizz\n",
      "58 58 58\n",
      "59 59 59\n",
      "60 fizzbuzz fizzbuzz\n",
      "61 61 61\n",
      "62 62 62\n",
      "63 fizz fizz\n",
      "64 64 64\n",
      "65 buzz buzz\n",
      "66 fizz fizz\n",
      "67 67 67\n",
      "68 68 68\n",
      "69 fizz fizz\n",
      "70 buzz buzz\n",
      "71 71 71\n",
      "72 fizz fizz\n",
      "73 73 73\n",
      "74 74 74\n",
      "75 fizzbuzz fizzbuzz\n",
      "76 76 76\n",
      "77 77 77\n",
      "78 fizz fizz\n",
      "79 79 79\n",
      "80 80 buzz\n",
      "81 fizz fizz\n",
      "82 82 82\n",
      "83 83 83\n",
      "84 fizz fizz\n",
      "85 fizz buzz\n",
      "86 86 86\n",
      "87 fizz fizz\n",
      "88 88 88\n",
      "89 89 89\n",
      "90 fizzbuzz fizzbuzz\n",
      "91 91 91\n",
      "92 92 92\n",
      "93 fizz fizz\n",
      "94 94 94\n",
      "95 fizz buzz\n",
      "96 fizz fizz\n",
      "97 97 97\n",
      "98 98 98\n",
      "99 fizz fizz\n",
      "100 buzz buzz\n",
      "96 / 1000\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "\n",
    "for n in range(1, 101):\n",
    "    x = binary_encode(n)\n",
    "    predicted = argmax(feed_forward(network, x)[-1])\n",
    "    actual = argmax(fizz_buzz_encode(n))\n",
    "    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "    print(n, labels[predicted], labels[actual])\n",
    "    \n",
    "    if predicted == actual:\n",
    "        num_correct += 1\n",
    "    \n",
    "print(num_correct, '/', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit ('data-science': pipenv)",
   "language": "python",
   "name": "python38532bitdatasciencepipenv068ffcbeb4194e50aa43c9cde15e24a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
