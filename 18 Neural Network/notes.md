# Neural Network

    y = w1x1  + w2x2 + wnxn + bias
    z = Activation(y)
    
    * Activation 
   
        sigmoid
             
            Activation (1/1+e^-y
            
        ReLu ( Rectified Linear Unit)
        softplus
        
        