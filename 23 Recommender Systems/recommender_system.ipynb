{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "recommender_system.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkarjun/Data-Science-from-Scratch/blob/master/23%20Recommender%20Systems/recommender_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krRgz3CuBtht"
      },
      "source": [
        "## Recommending system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMX1ecrvBthx"
      },
      "source": [
        "users_interests = [[\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\",\"Storm\", \"Cassandra\"],\n",
        "                   [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\",\"Postgres\"],\n",
        "                   [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
        "                   [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
        "                   [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
        "                   [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
        "                   [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
        "                   [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
        "                   [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
        "                   [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
        "                   [\"statistics\", \"R\", \"statsmodels\"],\n",
        "                   [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
        "                   [\"pandas\", \"R\", \"Python\"],\n",
        "                   [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
        "                   [\"libsvm\", \"regression\", \"support vector machines\"]]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrdTDPBvBz0a"
      },
      "source": [
        "### Recommending what's popular"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpRxAOs-Bthz"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDinON_B6kD"
      },
      "source": [
        "popular_interests = Counter(interest\r\n",
        "                            for user_interests in users_interests\r\n",
        "                            for interest in user_interests)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN40fWR8CQkT"
      },
      "source": [
        "from typing import List, Tuple"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGswFHV3CY5B"
      },
      "source": [
        "def most_popular_new_interests(\r\n",
        "                user_interests: List[str],\r\n",
        "                max_results: int = 5) -> List[Tuple[str, int]]:\r\n",
        "\r\n",
        "                suggestions = [(interest, frequency)\r\n",
        "                                for interest, frequency in popular_interests.most_common()\r\n",
        "                                if interest not in user_interests]\r\n",
        "\r\n",
        "                return suggestions[:max_results]\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYCALa37DKo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747a4038-131b-4a32-8b48-814158cf1c04"
      },
      "source": [
        "most_popular_new_interests(users_interests[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Python', 4), ('R', 4), ('Big Data', 3), ('Java', 3), ('statistics', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gieTdF_DsOp"
      },
      "source": [
        "### User-Based Collabrative Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLAIMDf3DyUm"
      },
      "source": [
        "unique_interests = sorted({interest for user_interests in users_interests\r\n",
        "                           for interest in user_interests})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpoBBMYEXy_f",
        "outputId": "898be82d-a4dd-4a41-d26c-d1f181eab7ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "unique_interests[:4]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Big Data', 'C++', 'Cassandra', 'HBase']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIZGg7XqX1am"
      },
      "source": [
        "def make_user_interest_vector(user_interests: List[str]) -> List[int]:\r\n",
        "  '''given a list of interests, produce a vector whose ith element is 1\r\n",
        "     if unique_interests[i] is in the list, 0 otherwise'''\r\n",
        "  return [1 if interest in user_interests else 0\r\n",
        "             for interest in unique_interests]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_interest_vectors = [make_user_interest_vector(user_interests)\n",
        "                         for user_interests in users_interests]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from linearalgebra import dot\n",
        "import math\n",
        "\n",
        "def cosine_similarity(v1, v2) -> float:\n",
        "    return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2))\n",
        "\n",
        "\n",
        "user_similarities = [[cosine_similarity(interest_vector_i,                                                      interest_vector_j)\n",
        "                      for interest_vector_j in user_interest_vectors]\n",
        "                      for interest_vector_i in user_interest_vectors]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n",
        "    pairs = [(other_user_id, similarity)\n",
        "            for other_user_id, similarity in enumerate(user_similarities[user_id])\n",
        "            if user_id != other_user_id and similarity > 0]\n",
        "\n",
        "    return sorted(pairs, key=lambda pair: pair[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9, 0.5669467095138409),\n",
              " (1, 0.3380617018914066),\n",
              " (8, 0.1889822365046136),\n",
              " (13, 0.1690308509457033),\n",
              " (5, 0.1543033499620919)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "most_similar_users_to(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def user_based_suggestions(user_id: int, include_current_interests: bool = False):\n",
        "    suggestions: Dict[str, float] = defaultdict(float)\n",
        "    \n",
        "    for other_user_id, similarity in most_similar_users_to(user_id):\n",
        "        for interest in users_interests[other_user_id]:\n",
        "            suggestions[interest] += similarity\n",
        "    \n",
        "    suggestions = sorted(suggestions.items(), \n",
        "                         key=lambda pairs: pairs[-1], reverse=True)\n",
        "    \n",
        "    if include_current_interests:\n",
        "        return suggestions\n",
        "    else:\n",
        "        return [(suggestions, weight)\n",
        "                for suggestions, weight in suggestions\n",
        "                if suggestions not in users_interests[user_id]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('MapReduce', 0.5669467095138409),\n",
              " ('MongoDB', 0.50709255283711),\n",
              " ('Postgres', 0.50709255283711),\n",
              " ('NoSQL', 0.3380617018914066),\n",
              " ('neural networks', 0.1889822365046136),\n",
              " ('deep learning', 0.1889822365046136),\n",
              " ('artificial intelligence', 0.1889822365046136),\n",
              " ('databases', 0.1690308509457033),\n",
              " ('MySQL', 0.1690308509457033),\n",
              " ('Python', 0.1543033499620919),\n",
              " ('R', 0.1543033499620919),\n",
              " ('C++', 0.1543033499620919),\n",
              " ('Haskell', 0.1543033499620919),\n",
              " ('programming languages', 0.1543033499620919)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "user_based_suggestions(0)"
      ]
    },
    {
      "source": [
        "## Item - Based collaborative Filtering"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "interest_user_matrix = [[user_interest_vector[j]\n",
        "                        for user_interest_vector in                   user_interest_vectors]\n",
        "                        for j, _ in enumerate(unique_interests)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "interest_user_matrix[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j)\n",
        "             for user_vector_j in interest_user_matrix]\n",
        "             for user_vector_i in interest_user_matrix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_similar_interests_to(interest_id: int):\n",
        "\n",
        "    similarities = interest_similarities[interest_id]\n",
        "\n",
        "    pairs = [(unique_interests[other_interest_id], similarity)\n",
        "             for other_interest_id, similarity in enumerate(similarities)\n",
        "             if interest_id != other_interest_id and similarity > 0]\n",
        "\n",
        "    return sorted(pairs, key=lambda pairs: pairs[-1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hadoop', 0.8164965809277261),\n",
              " ('Java', 0.6666666666666666),\n",
              " ('MapReduce', 0.5773502691896258),\n",
              " ('Spark', 0.5773502691896258),\n",
              " ('Storm', 0.5773502691896258),\n",
              " ('Cassandra', 0.4082482904638631),\n",
              " ('artificial intelligence', 0.4082482904638631),\n",
              " ('deep learning', 0.4082482904638631),\n",
              " ('neural networks', 0.4082482904638631),\n",
              " ('HBase', 0.3333333333333333)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "most_similar_interests_to(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def item_based_suggestions(user_id: int, include_current_interests: bool = False):\n",
        "\n",
        "    suggestions = defaultdict(float)\n",
        "\n",
        "    user_interest_vector = user_interest_vectors[user_id]\n",
        "\n",
        "    for interest_id, is_interested in enumerate(user_interest_vector):\n",
        "        if is_interested == 1:\n",
        "            similar_interests = most_similar_interests_to(interest_id)\n",
        "            for interest, similarity in similar_interests:\n",
        "                suggestions[interest] += similarity\n",
        "    \n",
        "    suggestions = sorted(suggestions.items(),\n",
        "                         key=lambda pair: pair[-1], reverse=True)\n",
        "    \n",
        "    if include_current_interests:\n",
        "        return suggestions\n",
        "    else:\n",
        "        return [(suggestion, weight)\n",
        "                for suggestion, weight in suggestions\n",
        "                if suggestions not in users_interests[user_id]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Spark', 4.146264369941973),\n",
              " ('Storm', 4.146264369941973),\n",
              " ('Hadoop', 3.9554550146924106),\n",
              " ('Cassandra', 3.547206724228547),\n",
              " ('Java', 3.3794454097708404),\n",
              " ('Big Data', 3.3794454097708404),\n",
              " ('HBase', 3.0461120764375074),\n",
              " ('MapReduce', 1.861807319565799),\n",
              " ('MongoDB', 1.3164965809277263),\n",
              " ('Postgres', 1.3164965809277263),\n",
              " ('NoSQL', 1.2844570503761732),\n",
              " ('MySQL', 0.5773502691896258),\n",
              " ('databases', 0.5773502691896258),\n",
              " ('Haskell', 0.5773502691896258),\n",
              " ('programming languages', 0.5773502691896258),\n",
              " ('artificial intelligence', 0.4082482904638631),\n",
              " ('deep learning', 0.4082482904638631),\n",
              " ('neural networks', 0.4082482904638631),\n",
              " ('C++', 0.4082482904638631),\n",
              " ('Python', 0.2886751345948129),\n",
              " ('R', 0.2886751345948129)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "item_based_suggestions(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def main():\n",
        "    \n",
        "    # Replace this with the locations of your files\n",
        "    \n",
        "    # This points to the current directory, modify if your files are elsewhere.\n",
        "    MOVIES = \"u.item\"   # pipe-delimited: movie_id|title|...\n",
        "    RATINGS = \"u.data\"  # tab-delimited: user_id, movie_id, rating, timestamp\n",
        "    \n",
        "    from typing import NamedTuple\n",
        "    \n",
        "    class Rating(NamedTuple):\n",
        "        user_id: str\n",
        "        movie_id: str\n",
        "        rating: float\n",
        "    \n",
        "    import csv\n",
        "    # We specify this encoding to avoid a UnicodeDecodeError.\n",
        "    # see: https://stackoverflow.com/a/53136168/1076346\n",
        "    with open(MOVIES, encoding=\"iso-8859-1\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"|\")\n",
        "        movies = {movie_id: title for movie_id, title, *_ in reader}\n",
        "    \n",
        "    # Create a list of [Rating]\n",
        "    with open(RATINGS, encoding=\"iso-8859-1\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        ratings = [Rating(user_id, movie_id, float(rating))\n",
        "                   for user_id, movie_id, rating, _ in reader]\n",
        "    \n",
        "    # 1682 movies rated by 943 users\n",
        "    assert len(movies) == 1682\n",
        "    assert len(list({rating.user_id for rating in ratings})) == 943\n",
        "    \n",
        "    import re\n",
        "    \n",
        "    # Data structure for accumulating ratings by movie_id\n",
        "    star_wars_ratings = {movie_id: []\n",
        "                         for movie_id, title in movies.items()\n",
        "                         if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n",
        "    \n",
        "    # Iterate over ratings, accumulating the Star Wars ones\n",
        "    for rating in ratings:\n",
        "        if rating.movie_id in star_wars_ratings:\n",
        "            star_wars_ratings[rating.movie_id].append(rating.rating)\n",
        "    \n",
        "    # Compute the average rating for each movie\n",
        "    avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id)\n",
        "                   for movie_id, title_ratings in star_wars_ratings.items()]\n",
        "    \n",
        "    # And then print them in order\n",
        "    for avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n",
        "        print(f\"{avg_rating:.2f} {movies[movie_id]}\")\n",
        "    \n",
        "    import random\n",
        "    random.seed(0)\n",
        "    random.shuffle(ratings)\n",
        "    \n",
        "    split1 = int(len(ratings) * 0.7)\n",
        "    split2 = int(len(ratings) * 0.85)\n",
        "    \n",
        "    train = ratings[:split1]              # 70% of the data\n",
        "    validation = ratings[split1:split2]   # 15% of the data\n",
        "    test = ratings[split2:]               # 15% of the data\n",
        "    \n",
        "    avg_rating = sum(rating.rating for rating in train) / len(train)\n",
        "    baseline_error = sum((rating.rating - avg_rating) ** 2\n",
        "                         for rating in test) / len(test)\n",
        "    \n",
        "    # This is what we hope to do better than\n",
        "    assert 1.26 < baseline_error < 1.27\n",
        "    \n",
        "    \n",
        "    # Embedding vectors for matrix factorization model\n",
        "    \n",
        "    from scratch.deep_learning import random_tensor\n",
        "    \n",
        "    EMBEDDING_DIM = 2\n",
        "    \n",
        "    # Find unique ids\n",
        "    user_ids = {rating.user_id for rating in ratings}\n",
        "    movie_ids = {rating.movie_id for rating in ratings}\n",
        "    \n",
        "    # Then create a random vector per id\n",
        "    user_vectors = {user_id: random_tensor(EMBEDDING_DIM)\n",
        "                    for user_id in user_ids}\n",
        "    movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM)\n",
        "                     for movie_id in movie_ids}\n",
        "    \n",
        "    \n",
        "    # Training loop for matrix factorization model\n",
        "    \n",
        "    from typing import List\n",
        "    import tqdm\n",
        "    from scratch.linear_algebra import dot\n",
        "    \n",
        "    def loop(dataset: List[Rating],\n",
        "             learning_rate: float = None) -> None:\n",
        "        with tqdm.tqdm(dataset) as t:\n",
        "            loss = 0.0\n",
        "            for i, rating in enumerate(t):\n",
        "                movie_vector = movie_vectors[rating.movie_id]\n",
        "                user_vector = user_vectors[rating.user_id]\n",
        "                predicted = dot(user_vector, movie_vector)\n",
        "                error = predicted - rating.rating\n",
        "                loss += error ** 2\n",
        "    \n",
        "                if learning_rate is not None:\n",
        "                    #     predicted = m_0 * u_0 + ... + m_k * u_k\n",
        "                    # So each u_j enters output with coefficent m_j\n",
        "                    # and each m_j enters output with coefficient u_j\n",
        "                    user_gradient = [error * m_j for m_j in movie_vector]\n",
        "                    movie_gradient = [error * u_j for u_j in user_vector]\n",
        "    \n",
        "                    # Take gradient steps\n",
        "                    for j in range(EMBEDDING_DIM):\n",
        "                        user_vector[j] -= learning_rate * user_gradient[j]\n",
        "                        movie_vector[j] -= learning_rate * movie_gradient[j]\n",
        "    \n",
        "                t.set_description(f\"avg loss: {loss / (i + 1)}\")\n",
        "    \n",
        "    learning_rate = 0.05\n",
        "    for epoch in range(20):\n",
        "        learning_rate *= 0.9\n",
        "        print(epoch, learning_rate)\n",
        "        loop(train, learning_rate=learning_rate)\n",
        "        loop(validation)\n",
        "    loop(test)\n",
        "    \n",
        "    \n",
        "    from scratch.working_with_data import pca, transform\n",
        "    \n",
        "    original_vectors = [vector for vector in movie_vectors.values()]\n",
        "    components = pca(original_vectors, 2)\n",
        "    \n",
        "    ratings_by_movie = defaultdict(list)\n",
        "    for rating in ratings:\n",
        "        ratings_by_movie[rating.movie_id].append(rating.rating)\n",
        "    \n",
        "    vectors = [\n",
        "        (movie_id,\n",
        "         sum(ratings_by_movie[movie_id]) / len(ratings_by_movie[movie_id]),\n",
        "         movies[movie_id],\n",
        "         vector)\n",
        "        for movie_id, vector in zip(movie_vectors.keys(),\n",
        "                                    transform(original_vectors, components))\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}